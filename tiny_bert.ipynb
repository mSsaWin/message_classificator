{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.onnx import FeaturesManager\n",
    "\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт класса для обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SsaWin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from text_preproccessor import TextPreproccessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура данных для датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer_path=\"cointegrated/rubert-tiny2\"):\n",
    "        super(DataSet, self).__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "        self.text_preprocessor = TextPreproccessor()\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        t = self.tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = t['input_ids']\n",
    "        token_type_ids = t['token_type_ids']\n",
    "        attention_mask = t['attention_mask']\n",
    "        \n",
    "        return input_ids.squeeze(), token_type_ids.squeeze(), attention_mask.squeeze()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        text = self.text_preprocessor.preproccess_text(text)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = self.tokenize(text)\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyBert Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBertClassificator(nn.Module):\n",
    "    def __init__(self, num_labels=1, dropout=0.25, embedding_size=312, model=\"cointegrated/rubert-tiny2\"):\n",
    "        super(TinyBertClassificator, self).__init__()\n",
    "\n",
    "        self.bert_model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=num_labels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(embedding_size, num_labels)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.reLU = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TinyBertClassificator, train_dataloader: DataLoader, loss_f, optimizer, batch_size=4, train=True):\n",
    "    model.train(train)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids, token_type_ids, attention_mask, labels = (t.cpu() for t in batch)\n",
    "\n",
    "        output = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "        batch_loss = loss_f(output, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()  # обнуляем градиенты\n",
    "            batch_loss.backward()  # вычисляем градиенты\n",
    "            optimizer.step()  # подправляем параметры\n",
    "\n",
    "        train_loss = batch_loss.item()\n",
    "    \n",
    "    return train_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "def test(model: TinyBertClassificator, test_dataloader: DataLoader, loss_f, batch_size=4):\n",
    "    total_loss_test = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            input_ids, token_type_ids, attention_mask, labels = (t.cpu() for t in batch)\n",
    "\n",
    "            output = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "            predicts.extend(output.argmax(-1).tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "\n",
    "            batch_loss = loss_f(output, labels)\n",
    "            total_loss_test += batch_loss.item()\n",
    "\n",
    "    total_loss_test = total_loss_test / len(test_dataloader)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, predicts)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, predicts)\n",
    "    f1 = f1_score(y_true, predicts, average='weighted')\n",
    "\n",
    "    metrics = {\"Accuracy\": accuracy,\n",
    "            \"Balanced_accuracy\": balanced_accuracy,\n",
    "            \"F1-score\": f1,\n",
    "            \"Test loss\": total_loss_test }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_excel_file(file_path: str) -> pd.DataFrame:\n",
    "    return pd.read_excel(file_path).dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение данных из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_FILEPATH = 'texts.xlsx'\n",
    "\n",
    "texts_df = get_texts_from_excel_file(TEXTS_FILEPATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = texts_df['name'].value_counts()\n",
    "\n",
    "#Считаем встречаемость классов в датасете\n",
    "name2count = {name:count for name,count in zip(class_counts.index, class_counts.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавляем в табличку столбец вхождений класса\n",
    "texts_df['class_count'] = texts_df['name'].apply(lambda x: name2count[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = texts_df[texts_df['class_count'] > 1][['description', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "name2class_id = {name: idx for idx, name in enumerate(data['name'].unique())}\n",
    "#Добавляем в табличку столбец с индексами классов\n",
    "data['class_id'] = data['name'].apply(lambda x: name2class_id[x])\n",
    "data.drop(['name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['description'].values\n",
    "y = data['class_id'].values\n",
    "#Делим датасет на тренировочную и тестовую выборки, учитывая распределение классов\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4                              #Размер батча\n",
    "epochs = 40                                 #Итерации обучения\n",
    "n_classes = len(data['class_id'].unique())  #Количество классов в датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем объекты тренировочного и тестового датасетов\n",
    "train_dataset = DataSet(X_train, y_train)\n",
    "test_dataset = DataSet(X_test, y_test)\n",
    "\n",
    "#Оборачиваем в dataloader-ы\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1110, in _get_module\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.ernie.configuration_ernie'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Local\\Temp\\ipykernel_19280\\2809425192.py\", line 1, in <module>\n",
      "    model = TinyBertClassificator(n_classes).cpu()\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Local\\Temp\\ipykernel_19280\\4269983073.py\", line 5, in __init__\n",
      "    self.bert_model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=num_labels)\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 462, in from_pretrained\n",
      "    \"\"\"\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 608, in keys\n",
      "    (\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 609, in <listcomp>\n",
      "    self._load_attr_from_module(key, self._config_mapping[key]),\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 605, in _load_attr_from_module\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 554, in getattribute_from_module\n",
      "    self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1100, in __getattr__\n",
      "  File \"c:\\Users\\SsaWin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1112, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.ernie.configuration_ernie because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.ernie.configuration_ernie'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\SsaWin\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model = TinyBertClassificator(n_classes).cpu()\n",
    "criterion = nn.CrossEntropyLoss().cpu()\n",
    "optimizer = Adam(model.parameters(), lr=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m table \u001b[39m=\u001b[39m PrettyTable([\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBalanced_accuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mF1-score\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTest loss\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, criterion,\n\u001b[0;32m      5\u001b[0m                        optimizer, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m      6\u001b[0m     metrics \u001b[39m=\u001b[39m test(model, test_dataloader, criterion, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m      8\u001b[0m     table\u001b[39m.\u001b[39madd_row([metrics[\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m], metrics[\u001b[39m\"\u001b[39m\u001b[39mBalanced_accuracy\u001b[39m\u001b[39m\"\u001b[39m], metrics[\u001b[39m\"\u001b[39m\u001b[39mF1-score\u001b[39m\u001b[39m\"\u001b[39m], metrics[\u001b[39m\"\u001b[39m\u001b[39mTest loss\u001b[39m\u001b[39m\"\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "table = PrettyTable([\"Accuracy\", \"Balanced_accuracy\", \"F1-score\", \"Test loss\"])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_dataloader, criterion,\n",
    "                       optimizer, batch_size=batch_size)\n",
    "    metrics = test(model, test_dataloader, criterion, batch_size=batch_size)\n",
    "    \n",
    "    table.add_row([metrics[\"Accuracy\"], metrics[\"Balanced_accuracy\"], metrics[\"F1-score\"], metrics[\"Test loss\"]])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Accuracy</th>\n",
       "            <th>Balanced_accuracy</th>\n",
       "            <th>F1-score</th>\n",
       "            <th>Test loss</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+-------------------+----------+-----------+\n",
       "| Accuracy | Balanced_accuracy | F1-score | Test loss |\n",
       "+----------+-------------------+----------+-----------+\n",
       "+----------+-------------------+----------+-----------+"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5355afa84ca9633534ce9b412c10717d9edcb1ff9ccf8f00101414d78fa9f67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
