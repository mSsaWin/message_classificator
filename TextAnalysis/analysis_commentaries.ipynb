{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16024bf",
   "metadata": {},
   "source": [
    "##### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9023e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint  # сохранение весов\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor # автоматическое отслеживание lr\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping  # ранние остановки\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"FutureWarning\")\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c125453",
   "metadata": {},
   "source": [
    "##### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a1f90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_file_from_csv(path: str) -> pd.DataFrame:\n",
    "#     return pd.read_csv(path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30443cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = 'data.csv'\n",
    "\n",
    "# data = get_file_from_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c7adc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a9fa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f5c90",
   "metadata": {},
   "source": [
    "##### Уберем нейтральные отзывы и маловстречающиеся оценки -> 0, 3, 7, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e74e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# excluded_grades = [0, 3 ,7 ,9]\n",
    "\n",
    "# data = data[~data.Rating.isin(excluded_grades)]\n",
    "\n",
    "# data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4eca2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Добавим колонку с оценками на тональность и заменим значения 0 - negative, 1 - positive\n",
    "\n",
    "# sentiments_decriptor = { 'negative': 0, 'positive': 1 }\n",
    "\n",
    "# data = data.rename(columns={'Rating': 'sentiment'})\n",
    "\n",
    "# data.loc[data['sentiment'] <= 2, 'sentiment'] = sentiments_decriptor['negative']\n",
    "# data.loc[data['sentiment'] >= 4, 'sentiment'] = sentiments_decriptor['positive']\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1b206",
   "metadata": {},
   "source": [
    "##### Определение устройства для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a03b143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0850d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# data = get_file_from_csv('small_data.csv')\n",
    "# data.rename(columns={'Review': 'text', 'sentiment': 'label'}, inplace=True)\n",
    "# data\n",
    "# train_data, test_data = train_test_split(data, test_size=0.3)\n",
    "# train_data, val_data = train_test_split(data, test_size=0.3)\n",
    "# # data['sentiment'].value_counts()\n",
    "# train_data = pd.DataFrame(train_data)\n",
    "# test_data = pd.DataFrame(test_data)\n",
    "# val_data = pd.DataFrame(val_data)\n",
    "# train_data['label'].value_counts()\n",
    "# test_data['label'].value_counts()\n",
    "# train_data.to_csv('train.csv', index=False)\n",
    "# test_data.to_csv('test.csv', index=False)\n",
    "# val_data.to_csv('val.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731a5ec",
   "metadata": {},
   "source": [
    "##### Класс преобработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca3c92b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SsaWin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import re\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "class TextPreproccessor:\n",
    "    def __init__(self):\n",
    "        self.__html_pattern = re.compile(\n",
    "            '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        self.__alphabet_pattern = r\"([^а-яА-Я!?:.)(\\s])|(\\n|\\r)\"\n",
    "\n",
    "        self.__mystem = Mystem()  # for lemmatization\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        self.__russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "    def __remove_html(self, text: str) -> str:\n",
    "        return re.sub(self.__html_pattern, '', text)\n",
    "\n",
    "    def __filter_symbols(self, text: str) -> str:\n",
    "        result = re.sub(self.__alphabet_pattern, '', text)\n",
    "        result = re.sub(r'[\\t\\r\\n]', '', result)\n",
    "        result = re.sub(r'(\\s{2,})', ' ', result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __lemmatization(self, text: str) -> str:\n",
    "\n",
    "        return ''.join(self.__mystem.lemmatize(text))\n",
    "\n",
    "    def __strip(self, text: str) -> str:\n",
    "        return text.strip(' ')\n",
    "\n",
    "    def __lower(self, text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    def __remove_stopwords_and_lemmatization(self, text: str) -> str:\n",
    "        words = text.split(' ')\n",
    "\n",
    "        words_without_stopwords = []\n",
    "\n",
    "        for word in words:\n",
    "            if word not in self.__russian_stopwords:\n",
    "                words_without_stopwords.append(self.morph.parse(word)[0].normal_form)\n",
    "\n",
    "\n",
    "        return ' '.join(words_without_stopwords)\n",
    "    \n",
    "    def preproccess_text(self, text: str) -> str:\n",
    "        text = self.__remove_html(text)\n",
    "        text = self.__lower(text)\n",
    "        text = self.__strip(text)\n",
    "        text = text.replace('ё', 'e')\n",
    "        text = self.__filter_symbols(text)\n",
    "        text = self.__remove_stopwords_and_lemmatization(text)\n",
    "        # text = self.__lemmatization(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "859d3e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рама'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "res = morph.parse('раму')[0]\n",
    "res.normal_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da4a43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path=\"cointegrated/rubert-tiny2\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        df = self.read_csv(file_path)\n",
    "        \n",
    "        self.texts = df['text'].values.astype(str)\n",
    "        self.labels = df['label'].values.astype(int)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        self.text_preproccessor = TextPreproccessor()\n",
    "\n",
    "    def read_csv(self, path: str) -> pd.DataFrame:\n",
    "        return pd.read_csv(path).dropna()\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        t = self.tokenizer(text, padding='max_length',\n",
    "                           truncation=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = t['input_ids']\n",
    "        token_type_ids = t['token_type_ids']\n",
    "        attention_mask = t['attention_mask']\n",
    "\n",
    "        return input_ids.squeeze(), token_type_ids.squeeze(), attention_mask.squeeze()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text: str = self.texts[idx]\n",
    "        label: int = self.labels[idx]\n",
    "        \n",
    "        text = self.text_preproccessor.preproccess_text(text)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = self.tokenize(text)\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfe2db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 batch_size: int = 32,\n",
    "                 data_dir: str = './data/',\n",
    "                 num_workers: int = 12,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = CSVDataset(os.path.join(self.data_dir, 'train.csv'))\n",
    "        self.val_set = CSVDataset(os.path.join(self.data_dir, 'val.csv'))\n",
    "        self.test_set = CSVDataset(os.path.join(self.data_dir, 'test.csv'))\n",
    "\n",
    "    def train_dataloader(self):       \n",
    "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size, shuffle=True)#, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if __name__ == '__main__':\n",
    "            return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False)#, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False)#, num_workers=self.num_workers)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f7e7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticClassifier(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 model=\"cointegrated/rubert-tiny2\",\n",
    "                 out_channels=1,\n",
    "                 dropout=0.25,\n",
    "                 eta=3e-4,\n",
    "                 criterion = None,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=out_channels)\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.criterion = criterion if criterion is not None else nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.eta = eta\n",
    "\n",
    "        self.metrics = {'accuracy': Accuracy(task='binary').to(device)}\n",
    "\n",
    "        print('INIT MODEL')\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # print('FORWARD')\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
    "        return output\n",
    "\n",
    "\n",
    "    def shared_step(self, sample, stage):\n",
    "        # print('SHARED STEP', stage)\n",
    "        input_ids, token_type_ids, attention_mask, label = sample\n",
    "\n",
    "        logits = self.forward(input_ids, token_type_ids, attention_mask)\n",
    "        \n",
    "        # label = label.unsqueeze(1).float()\n",
    "        # print('logits', logits.shape)\n",
    "        # print('label', label.shape)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        loss = self.criterion(logits, label.unsqueeze(1).float())\n",
    "        # print('LOSS', loss.shape)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'accuracy': self.metrics[\"accuracy\"](preds, label)\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        loss = np.mean([x[\"loss\"].item() for x in outputs])\n",
    "        acc = np.mean([x[\"accuracy\"].item() for x in outputs])\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": loss,\n",
    "            f\"{stage}_acc\": acc\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # print('CONFIGURING OPTIMIZERS')\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.eta)\n",
    "\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=5\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"valid_loss\"\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'train')\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'valid')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, 'valid')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'test')\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, 'test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd368104",
   "metadata": {},
   "source": [
    "## свернул"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b50d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "dm = SentenceDataModule(batch_size=BATCH_SIZE)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3e4e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT MODEL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SemanticClassifier(\n",
       "  (model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "        (position_embeddings): Embedding(2048, 312)\n",
       "        (token_type_embeddings): Embedding(2, 312)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=312, out_features=1, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = SemanticClassifier()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d804f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath='models',\n",
    "        filename='{epoch}_{valid_acc:.2f}_{valid_loss:.2f}',\n",
    "        save_top_k=2,\n",
    "        monitor='valid_loss',\n",
    "        mode='min'\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    EarlyStopping(\n",
    "        monitor=\"valid_loss\",\n",
    "        min_delta=2e-4,\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "]\n",
    "\n",
    "LOG_PATH = './logs'\n",
    "logger = TensorBoardLogger(LOG_PATH, name='tiny_bert')\n",
    "\n",
    "CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7fc036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='cuda', \n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    resume_from_checkpoint=CHECKPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1a18752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env CUDA_LAUNCH_BLOCKING = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37054c86",
   "metadata": {},
   "source": [
    "## не свернул"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e610a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SsaWin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\SsaWin\\Desktop\\TextAnalysis\\models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 29.2 M\n",
      "1 | criterion | BCEWithLogitsLoss             | 0     \n",
      "2 | dropout   | Dropout                       | 0     \n",
      "------------------------------------------------------------\n",
      "29.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.776   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b898c8db158f4faf8a89fa8d0051be5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SsaWin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\SsaWin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93958844201b4ebb9a9375171a42b154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a62991f85d4dd8ab37cfab0721a649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477dc0ae4fd04617a19d920d98578bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a2e831b46423aa442c0a98a4d9ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc72759712d4a91a6e600cf379a9d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edfec83f4e44c77a4e307d344d56d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SsaWin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING = \"1\"\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c65f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "c:\\Users\\SsaWin\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12a39da070b44aab282dac6c3680711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.49533333333333335\n",
      "        test_loss           0.3488016105443239\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3488016105443239, 'test_acc': 0.49533333333333335}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: 6348: No such process\n",
      "kill: 6006: No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 6348\n",
    "!kill 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32975cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6348), started 3:08:47 ago. (Use '!kill 6348' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8f147327029bfeec\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8f147327029bfeec\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs/tiny_bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5355afa84ca9633534ce9b412c10717d9edcb1ff9ccf8f00101414d78fa9f67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
