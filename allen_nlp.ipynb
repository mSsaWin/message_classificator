{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.data import (\n",
    "    DataLoader,\n",
    "    DatasetReader,\n",
    "    Instance,\n",
    "    Vocabulary,\n",
    "    TextFieldTensors,\n",
    ")\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.data.fields import LabelField, TextField, Field\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.nn import util\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.training import Trainer, GradientDescentTrainer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "from allennlp.training.util import evaluate\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт класса для обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preproccessor import TextPreproccessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationExcelReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        max_tokens: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ClassificationExcelReader, self).__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self.token_indexers = token_indexers or {\n",
    "            \"tokens\": SingleIdTokenIndexer()}\n",
    "            \n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.text_preprocessor = TextPreproccessor()\n",
    "\n",
    "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {'text': text_field}\n",
    "        if label:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def __get_texts_from_excel_file(self, file_path: str) -> pd.DataFrame:\n",
    "        return pd.read_excel(file_path).dropna()\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        texts_df: pd.DataFrame = self.__get_texts_from_excel_file(file_path)\n",
    "        \n",
    "        for row in texts_df.itertuples(index=True):\n",
    "            text = self.text_preprocessor.preproccess_text(row.description)\n",
    "            label = row.name\n",
    "\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            if self.max_tokens:\n",
    "                tokens = tokens[: self.max_tokens]\n",
    "\n",
    "            text_field = TextField(tokens, self.token_indexers)\n",
    "            label_field = LabelField(label)\n",
    "\n",
    "            fields: Dict[str, Field] = {\n",
    "                \"text\": text_field, \n",
    "                \"label\": label_field\n",
    "            }\n",
    "\n",
    "            yield Instance(fields)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllenClassifier(Model):\n",
    "    def __init__(self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoder.get_output_dim(), num_labels),\n",
    "            torch.nn.Linear(encoder.get_output_dim(), num_labels),\n",
    "            torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        )\n",
    "\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n",
    "    def forward(self, text: TextFieldTensors, label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "\n",
    "        logits = self.classifier(encoded_text)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output = {'probs': probs}\n",
    "\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "\n",
    "            output['loss'] = torch.nn.functional.cross_entropy(logits, label)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции создания необходимых объектов для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def train_test_split(instances: List[Instance], test_size=0.2) -> Tuple[List[Instance], List[Instance]]:\n",
    "    test_count = int(len(instances) * test_size)\n",
    "\n",
    "    return instances[:-test_count], instances[-test_count:]\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    embedder = BasicTextFieldEmbedder(\n",
    "        {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)}\n",
    "    )\n",
    "    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
    "    return AllenClassifier(vocab, embedder, encoder)\n",
    "\n",
    "\n",
    "def build_data_loaders(train_data: List[Instance], test_data: List[Instance], batches_per_epoch=4) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_loader = SimpleDataLoader(train_data, batches_per_epoch, shuffle=True)\n",
    "    test_loader = SimpleDataLoader(test_data, batches_per_epoch, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def build_trainer(model: Model, train_loader: DataLoader, test_loader: DataLoader, num_epochs=40) -> Trainer:\n",
    "    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = AdamOptimizer(parameters)\n",
    "\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(train_data, test_data):\n",
    "    \n",
    "    vocab = build_vocab(train_data + test_data)\n",
    "\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    train_loader, test_loader = build_data_loaders(train_data, test_data)\n",
    "    train_loader.index_with(vocab)\n",
    "    test_loader.index_with(vocab)\n",
    "\n",
    "    trainer = build_trainer(model, train_loader, test_loader)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных из файла и их подготовка к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_FILEPATH = 'texts.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reader = ClassificationExcelReader()\n",
    "data = list(dataset_reader.read(TEXTS_FILEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7572 1893\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data)\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feb666b5f25450592018ea48e695a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/9465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcb3f8aae294917862540de5363e431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x0 and 10x0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m run_training_loop(train_data, test_data)\n",
      "Cell \u001b[0;32mIn[138], line 14\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[39m=\u001b[39m build_trainer(model, train_loader, test_loader)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/allennlp/training/gradient_descent_trainer.py:771\u001b[0m, in \u001b[0;36mGradientDescentTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     metrics, epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_train()\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[1;32m    773\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/allennlp/training/gradient_descent_trainer.py:793\u001b[0m, in \u001b[0;36mGradientDescentTrainer._try_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_epochs):\n\u001b[1;32m    792\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 793\u001b[0m     train_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(epoch)\n\u001b[1;32m    795\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_after_epochs_completed:\n\u001b[1;32m    796\u001b[0m         \u001b[39m# We're still catching up with the checkpoint, so we do nothing.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Note that we have to call _train_epoch() even when we know the epoch is skipped. We have to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[39m# time we train, even when starting from a checkpoint, so that we update the randomness\u001b[39;00m\n\u001b[1;32m    801\u001b[0m         \u001b[39m# generators in the same way each time.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/allennlp/training/gradient_descent_trainer.py:510\u001b[0m, in \u001b[0;36mGradientDescentTrainer._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_amp):\n\u001b[0;32m--> 510\u001b[0m     batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_outputs(batch, for_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    511\u001b[0m     batch_group_outputs\u001b[39m.\u001b[39mappend(batch_outputs)\n\u001b[1;32m    512\u001b[0m     loss \u001b[39m=\u001b[39m batch_outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/allennlp/training/gradient_descent_trainer.py:403\u001b[0m, in \u001b[0;36mGradientDescentTrainer.batch_outputs\u001b[0;34m(self, batch, for_training)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_outputs\u001b[39m(\u001b[39mself\u001b[39m, batch: TensorDict, for_training: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m    Does a forward pass on the given batch and returns the output dictionary that the model\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m    returns, after adding any specified regularization penalty to the loss (if training).\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     output_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pytorch_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m for_training:\n\u001b[1;32m    406\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "Cell \u001b[0;32mIn[136], line 27\u001b[0m, in \u001b[0;36mAllenClassifier.forward\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m     24\u001b[0m mask \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mget_text_field_mask(text)\n\u001b[1;32m     25\u001b[0m encoded_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(embedded_text, mask)\n\u001b[0;32m---> 27\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(encoded_text)\n\u001b[1;32m     29\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m output \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprobs\u001b[39m\u001b[39m'\u001b[39m: probs}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x0 and 10x0)"
     ]
    }
   ],
   "source": [
    "model = run_training_loop(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ямы и выбоины на тротуарах</td>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Нарушено дорожное покрытие (ямы) на дорогах в ...</td>\n",
       "      <td>0.017310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Несвоевременный (некачественный) текущий ремон...</td>\n",
       "      <td>0.017073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Длительное неисполнение заявок управляющей ком...</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Неудовлетворительное содержание контейнерной п...</td>\n",
       "      <td>0.016519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Водоотведение</td>\n",
       "      <td>0.001162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Незавершенное благоустройство после сдачи стро...</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Некачественное предоставление услуг доступа в ...</td>\n",
       "      <td>0.001105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Неудовлетворительное качество энергоснабжения ...</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Плохое материально-техническое оснащение учреж...</td>\n",
       "      <td>0.000711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label      prob\n",
       "1                           Ямы и выбоины на тротуарах  0.021635\n",
       "2    Нарушено дорожное покрытие (ямы) на дорогах в ...  0.017310\n",
       "7    Несвоевременный (некачественный) текущий ремон...  0.017073\n",
       "9    Длительное неисполнение заявок управляющей ком...  0.016900\n",
       "16   Неудовлетворительное содержание контейнерной п...  0.016519\n",
       "..                                                 ...       ...\n",
       "125                                      Водоотведение  0.001162\n",
       "160  Незавершенное благоустройство после сдачи стро...  0.001139\n",
       "156  Некачественное предоставление услуг доступа в ...  0.001105\n",
       "126  Неудовлетворительное качество энергоснабжения ...  0.000942\n",
       "154  Плохое материально-техническое оснащение учреж...  0.000711\n",
       "\n",
       "[161 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "vocab = model.vocab\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader)\n",
    "\n",
    "output = predictor.predict(\n",
    "    \"<p>Тротуары отсутствуют, <strong>Алексеевский городской округ, посёлок Ольминского, 13</strong></p>\")\n",
    "\n",
    "table_dict = {'label': [], 'prob': []}\n",
    "for label_id, prob in enumerate(output[\"probs\"]):\n",
    "    table_dict['label'].append(vocab.get_token_from_index(label_id, \"labels\"))\n",
    "    table_dict[\"prob\"].append(prob)\n",
    "    \n",
    "table = pd.DataFrame(table_dict).sort_values(by = \"prob\", ascending = 0)\n",
    "\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.26.2.tar.gz (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m527.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp310-cp310-macosx_10_9_x86_64.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.3/358.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp310-cp310-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-macosx_10_9_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai: filename=openai-0.26.2-py3-none-any.whl size=67444 sha256=d6e59a29d7ced3e62bcaf1cc00b1f684604ed9ed3a4b299a8cb7ef0fef2b0768\n",
      "  Stored in directory: /Users/fil-gnezdilov/Library/Caches/pip/wheels/71/4c/2e/6ac22e55ba28a43a664aa6e4448412d79d2034294aec74c047\n",
      "Successfully built openai\n",
      "Installing collected packages: multidict, frozenlist, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 frozenlist-1.3.3 multidict-6.0.4 openai-0.26.2 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
